# [ID] Uso de Ollama en Docker

**Estado:** [Propuesto | Aceptado | Deprecado]
**Fecha:** 2026-01-18

## Contexto
Necesitamos inferencia de IA en hardware limitado (RPi 5)

## Decisión
Usar Ollama containerizado

## Consecuencias
Ahorro de RAM vs correr modelos crudos en PyTorch. Facilidad de actualización.
